---
date: 2016-01-05
layout: post
title: Openstack HA 服务部署总结
categories: openstack
tags: pacemaker
---

###HA服务说明

1. openstack中无状态和有状态服务说明
无状态的服务，请求无上下文关系，请求间相互独立，无状态服务的高可用性采用Active/Active 模式。
有状态的服务，请求有上下文关系，请求会依赖上个请求的响应结果，有状态服务的高可用性采用Active/Passive 模式。

        OpenStack无状态的服务包括：  
        nova-api
        nova-conductor
        nova-novncproxy
        nova-scheduler
        keystone
        neutron-server
        glance-api
        glance-registry
        cinder-api
        cinder-scheduler
        
        OpenStack有状态的服务包括：
        nova-consoleauth
        neutron-dhcp-agent
        neutron-l3-agent
        neutron-metadata-agent
        neutron-openvswitch-agent 

2. openstack无状态服务HA配置（Active/Active）

    所有OpenStack无状态服务安装到至少两个节点上，并且使用负载平衡和虚拟IP(HAProxy & pacemaker)动态调度服务。
    虚拟IP配置注意点如下：
    * 配置OpenStack Identity endpoints的时候，使用虚拟IP。
    * 所有的OpenStack配置文件都修改成虚拟 IP。

3. openstack有状态服务HA配置（Active/Passive）
所有OpenStack有状态服务安装到至少两个节点上，把有状态服务作为资源由pacemaker管理，pacemaker会保证同时只有一个服务在运行，
如果服务出现故障，会自动启用备用节点上的服务。

4. 基础服务HA配置（mysql，rabbitmq）
    * 使用Galera实现MySQL Active/Active集群配置
    Galera是一个MySQL(也支持MariaDB，Percona)的同步多主集群软件。主要功能有:同步复制，并行复制，所有节点可以同时读写数据库，
    新节点加入数据自动复制， 失效节点自动被清除， 可以直接连接集群，使用感受上与MySQL完全一致。
    可以安装带有wsrep(Write Set REPlication)补丁的MySQL版本。由于Wsrep API支持同步复制，很适合用于配置OpenStack里MySQL高可用性。
    首先可以启动一个实例来创建集群，然后其它的MySQL实例连接到这个集群。因为Galera mysql用到了领导机选举机制quorum，所以对于这个service来说至少需要三个控制结点。

    * RabbitMQ Active/Active集群配置
    RabbitMQ是默认的AMQP服务器，RabbitMQ服务高可用性包括三个步骤: 安装RabbitMQ ，为HA队列配置RabbitMQ ，使用Rabbit HA队列配置OpenStack服务。
    必须确保所有的结点有相同的Erlang cookie文件。可以停止所有的RabbitMQ，然后从rabbit一台服务器复制cookie到其它服务器上，并且必须配置openstack组件使用至少两个RabbitMQ结点。
 
###Pacemaker

1. 安装配置

    ```c
    # yum install crmsh pacemaker
    # vi /etc/corosync/corosync.conf
    totem {
          version: 2
          token: 10000
         token_retransmits_before_loss_const: 10
         vsftype: none
         secauth: on
         threads: 0
         rrp_mode: active
         interface {
                 ringnumber: 0
                 bindnetaddr: 10.167.230.0
                 broadcast: yes
                 mcastport: 5405
         }
         transport: udpu
    }
    nodelist {
            node {
                    ring0_addr: 10.167.230.212
                    nodeid: 1
            }
            node {
                    ring0_addr: 10.167.230.213
                    nodeid: 2
            }
    }
    
    amf {
         mode: disabled
    }
    
    service {
            # Load the Pacemaker Cluster Resource Manager
            ver:       1
            name:      pacemaker
    }
    
    aisexec {
            user:   root
            group:  root
    }
    logging {
            fileline: off
            to_stderr: yes
            to_logfile: yes
            logfile: /var/log/cluster/corosync.log
            to_syslog: no
            syslog_facility: daemon
            debug: off
            timestamp: on
            logger_subsys {
                    subsys: AMF
                    debug: off
                    tags: enter|leave|trace1|trace2|trace3|trace4|trace6
            }
    }
    quorum {
            provider: corosync_votequorum
            expected_votes: 7
            wait_for_all: 1
            last_man_standing: 1
            last_man_standing_window: 10000
    }
    ```

2. 启动pacemaker 和 corosync服务

    ```c
    # mv /dev/{random,random.bak} 
    # ln -s /dev/urandom /dev/random 
    # corosync-keygen
    拷贝 /etc/corosync/authkey 到副控节点
    # systemctl enable corosync pacemaker
    # systemctl start corosync pacemaker
    ```

3. 配置服务资源

    ```c
    #crm configure
    property no-quorum-policy="ignore" \ 
      pe-warn-series-max="1000" \        
      pe-input-series-max="1000" \
      pe-error-series-max="1000" \
      cluster-recheck-interval="5min"
    property stonith-enabled=false
    primitive p_api-ip ocf:heartbeat:IPaddr2 \
    params ip="10.167.230.214" cidr_netmask="24" \
    op monitor interval="30s"
    primitive p_openstack-nova-consoleauth systemd:openstack-nova-consoleauth
    primitive p_neutron-metadata-agent systemd:neutron-openvswitch-agent
    primitive p_neutron-metadata-agent systemd:neutron-metadata-agent
    ```
    
###haproxy服务安装

1. 安装
    
    ```c
    # yum -y install haproxy
    ```

2. 配置
    
    ```c
    # chkconfig haproxy on
    # vi /etc/haproxy/haproxy.cfg
    修改内容：
    global
      log     127.0.0.1 local3
      chroot  /var/lib/haproxy
      daemon
      group  haproxy
      maxconn  4000
      pidfile  /var/run/haproxy.pid
      user  haproxy
    
    defaults
      log  global
      maxconn  4000
      option   redispatch
      log global
      option httplog
      retries  3
      timeout  http-request 10s
      timeout  queue 1m
      timeout  connect 10s
      timeout  client 1m
      timeout  server 1m
      timeout  check 10s
    
    listen dashboard_cluster
      bind :80
      balance  source
      option  tcpka
      option  httpchk
      option  tcplog
      server controller 10.167.230.212:81 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:81 check inter 2000 rise 2 fall 5
    
    listen keystone_admin_cluster
      bind :35357
      balance  source
      option  tcpka
      option  httpchk
      option  tcplog
      server controller 10.167.230.212:35356 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:35356 check inter 2000 rise 2 fall 5
    
    listen keystone_public_internal_cluster
      bind :5000
      balance  source
      option  tcpka
      option  httpchk
      option  tcplog
      server controller 10.167.230.212:15000 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:15000 check inter 2000 rise 2 fall 5
    
    listen glance_api_cluster
      bind :9292
      balance  source
      option  tcpka
      option  httpchk
      option  tcplog
      server controller 10.167.230.212:19292 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:19292 check inter 2000 rise 2 fall 5
    
    listen glance_registry_cluster
      bind :9191
      balance  source
      option  tcpka
      option  tcplog
      server controller 10.167.230.212:19191 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:19191 check inter 2000 rise 2 fall 5
    
    listen cinder_api_cluster
      bind :8776
      balance  source
      option  tcpka
      option  httpchk
      option  tcplog
      server controller 10.167.230.212:18776 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:18776 check inter 2000 rise 2 fall 5
    
    listen neutron_api_cluster
      bind :9696
      balance  source
      option  tcpka   
      option  httpchk
      option  tcplog
      server controller 10.167.230.212:19696 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:19696 check inter 2000 rise 2 fall 5
    
    listen ceilometer_api_cluster
      bind :8777
      balance  source
      option  tcpka
      option  tcplog
      server controller 10.167.230.212:18777 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:18777 check inter 2000 rise 2 fall 5
    
    listen mongo_rs0
      bind :27016
      balance  source
      option  tcpka
      option  tcplog
      server controller 10.167.230.212:27017 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:27017 check inter 2000 rise 2 fall 5
    
    listen nova_compute_api_cluster
      bind :8774
      balance  source
      option  tcpka
      option  httpchk
      option  tcplog
      server controller 10.167.230.212:18774 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:18774 check inter 2000 rise 2 fall 5
    
    listen spice_cluster
      bind :6080
      balance  source
      option  tcpka
      option  tcplog
      server controller 10.167.230.212:6081 check inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:6081 check inter 2000 rise 2 fall 5
    
    listen galera_cluster
      bind :3307
      balance  source
      option  httpchk
      server controller 10.167.230.212:3306 check port 9200 inter 2000 rise 2 fall 5
      server controller-standby 10.167.230.213:3306 backup check port 9200 inter 2000 rise 2 fall 5
      server cmp09 10.167.230.219:3306 backup check port 9200 inter 2000 rise 2 fall 5
    ```

###mariadb-galera集群安装

1.  安装mariadb
```c
# yum install -y mariadb-galera-server xinetd rsync
```

2. 配置clustercheck
```c
# vi /etc/sysconfig/clustercheck
增加内容如下：
MYSQL_USERNAME="clustercheck"
MYSQL_PASSWORD={PASSWORD}
MYSQL_HOST="localhost"
MYSQL_PORT="3306"
```

3. 配置galera-monitor
```c
# vi /etc/xinetd.d/galera-monitor 
增加内容如下：
service galera-monitor
{
   port = 9200
   disable = no
   socket_type = stream
   protocol = tcp
   wait = no
   user = root
   group = root
   groups = yes
   server = /usr/bin/clustercheck
   type = UNLISTED
   per_source = UNLIMITED
   log_on_success =
   log_on_failure = HOST
   flags = REUSE
}
```

4. 数据库中增加clustercheck用户
```c
# systemctl start mysqld
# mysql -uroot -pfnst@cloud -e "CREATE USER 'clustercheck'@'localhost' IDENTIFIED BY 'PASSWORD';"
# systemctl stop mysqld
```

5. 启动xinted服务
```c
# systemctl daemon-reload
# systemctl enable xinetd
# systemctl start xinetd
```

6. 设置galera配置文件
```c
# vi /etc/my.cnf.d/galera.cnf
内容如下：
[mysqld]
skip-name-resolve=1
binlog_format=ROW
default-storage-engine=innodb
innodb_autoinc_lock_mode=2
innodb_locks_unsafe_for_binlog=1
max_connections=2048
query_cache_size=0
query_cache_type=0
bind_address=NODE_IP
wsrep_provider=/usr/lib64/galera/libgalera_smm.so
wsrep_cluster_name="galera_cluster"
wsrep_cluster_address="gcomm://PRIMARY_NODE_IP, SECONDARY_NODE_IP, TERTIARY_NODE_IP"
#注意：第一个节点启动时需要把wsrep_cluster_address 修改为"gcomm://"
wsrep_slave_threads=1
wsrep_certify_nonPK=1
wsrep_max_ws_rows=131072
wsrep_max_ws_size=1073741824
wsrep_debug=0
wsrep_convert_LOCK_to_trx=0
wsrep_retry_autocommit=1
wsrep_auto_increment_control=1
wsrep_drupal_282555_workaround=0
wsrep_causal_reads=0
wsrep_notify_cmd=
wsrep_sst_method=rsync
```

7. 启动集群服务
启动各个节点maraidb服务
注意：第一个节点启动时需要把wsrep_cluster_address 修改为"gcomm://"
```c
# systemctl start mariadb
全部启动成功后，修改第一个节点地址为
wsrep_cluster_address="gcomm://PRIMARY_NODE_IP, SECONDARY_NODE_IP, TERTIARY_NODE_IP"
再重新启动第一个节点maraiadb
# systemctl restart mariadb
```
